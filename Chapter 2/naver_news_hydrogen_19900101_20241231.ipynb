{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSE5h2YEkCwY",
        "outputId": "c34e2aa0-0b73-4323-bd26-a3cd0247bb8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "페이지 1 크롤링 중입니다.\n",
            "뉴스 본문 텍스트 크롤링을 시작합니다!\n",
            "페이지 1 크롤링 중입니다.\n",
            "뉴스 본문 텍스트 크롤링을 시작합니다!\n",
            "페이지 1 크롤링 중입니다.\n",
            "뉴스 본문 텍스트 크롤링을 시작합니다!\n",
            "페이지 1 크롤링 중입니다.\n",
            "뉴스 본문 텍스트 크롤링을 시작합니다!\n",
            "페이지 1 크롤링 중입니다.\n",
            "뉴스 본문 텍스트 크롤링을 시작합니다!\n",
            "페이지 1 크롤링 중입니다.\n",
            "뉴스 본문 텍스트 크롤링을 시작합니다!\n",
            "페이지 1 크롤링 중입니다.\n",
            "뉴스 본문 텍스트 크롤링을 시작합니다!\n",
            "페이지 1 크롤링 중입니다.\n",
            "뉴스 본문 텍스트 크롤링을 시작합니다!\n",
            "페이지 1 크롤링 중입니다.\n",
            "뉴스 본문 텍스트 크롤링을 시작합니다!\n",
            "페이지 1 크롤링 중입니다.\n",
            "뉴스 본문 텍스트 크롤링을 시작합니다!\n",
            "페이지 1 크롤링 중입니다.\n",
            "뉴스 본문 텍스트 크롤링을 시작합니다!\n",
            "페이지 1 크롤링 중입니다.\n",
            "뉴스 본문 텍스트 크롤링을 시작합니다!\n",
            "페이지 1 크롤링 중입니다.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from requests import get\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from datetime import datetime, timedelta, date\n",
        "from time import sleep\n",
        "import random\n",
        "from openpyxl import Workbook\n",
        "1\n",
        "# 사용자 입력\n",
        "keyword = '수소'\n",
        "start_date = '1990.01.01'\n",
        "end_date = '2024.12.31'\n",
        "\n",
        "# 엑셀 파일 생성\n",
        "wb = Workbook()\n",
        "ws = wb.active\n",
        "ws.title = keyword\n",
        "\n",
        "# 엑셀 열 너비 조절\n",
        "ws.column_dimensions['A'].width = 20\n",
        "ws.column_dimensions['B'].width = 20\n",
        "ws.column_dimensions['C'].width = 60\n",
        "ws.column_dimensions['D'].width = 120\n",
        "ws.column_dimensions['E'].width = 50\n",
        "ws.column_dimensions['F'].width = 20  # 언론사명\n",
        "\n",
        "# 엑셀 파일에 헤더 설정\n",
        "ws.append(['날짜', '키워드', '네이버 뉴스제목', '네이버뉴스 내용', '네이버뉴스 링크', '언론사명'])\n",
        "\n",
        "# 헤더 설정 - 웹사이트에 접속할 때 사용할 사용자 정보\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',\n",
        "    'Connection': 'keep-alive',\n",
        "    'Upgrade-Insecure-Requests': '1',\n",
        "    'Referer': 'https://www.naver.com/',\n",
        "}\n",
        "\n",
        "# 날짜 범위를 3개월씩 나누는 함수\n",
        "def get_date_ranges(start_date, end_date, delta=timedelta(days=90)):\n",
        "    start_dt = datetime.strptime(start_date, '%Y.%m.%d')\n",
        "    end_dt = datetime.strptime(end_date, '%Y.%m.%d')\n",
        "\n",
        "    ranges = []\n",
        "    while start_dt < end_dt:\n",
        "        temp_end_dt = min(start_dt + delta, end_dt)\n",
        "        ranges.append((start_dt.strftime('%Y.%m.%d'), temp_end_dt.strftime('%Y.%m.%d')))\n",
        "        start_dt = temp_end_dt + timedelta(days=1)\n",
        "    return ranges\n",
        "\n",
        "# 문자열에서 불법 문자를 제거하는 함수 (정규 표현식으로 불법 문자 필터링)\n",
        "def clean_string(value):\n",
        "    # Excel에서 허용되지 않는 문자 및 잘못된 유니코드 문자 필터링\n",
        "    ILLEGAL_CHARACTERS_RE = re.compile(r'[\\x00-\\x1F\\x7F-\\x9F\\uD800-\\uDFFF]')\n",
        "    if isinstance(value, str):\n",
        "        return ILLEGAL_CHARACTERS_RE.sub('', value)\n",
        "    return value\n",
        "\n",
        "# 뉴스 크롤링 함수 (최대 200페이지 크롤링)\n",
        "def fetch_news_by_date_range(keyword, start_date, end_date):\n",
        "    naver_news_links = []\n",
        "\n",
        "    # 최대 200페이지(2000개 기사) 크롤링\n",
        "    for page in range(1, 3001, 10):  # 1페이지부터 200페이지까지 (페이지당 10개 기사)\n",
        "        print(f\"페이지 {page // 10 + 1} 크롤링 중입니다.\")\n",
        "        url = f\"https://search.naver.com/search.naver?where=news&query={keyword}&sm=tab_opt&sort=0&photo=0&field=0&pd=3&ds={start_date}&de={end_date}&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3Afrom{start_date.replace('.', '')}to{end_date.replace('.', '')}&start={page}\"\n",
        "\n",
        "        try:\n",
        "            response = get(url, headers=headers)\n",
        "            if response.status_code == 200:\n",
        "                soup = bs(response.text, 'html.parser')\n",
        "                news_items = soup.select('a.info')\n",
        "                if not news_items:\n",
        "                    break\n",
        "                for item in news_items:\n",
        "                    link = item['href']\n",
        "                    if 'news.naver.com' in link:\n",
        "                        naver_news_links.append(link)\n",
        "                sleep(random.uniform(3, 5))  # 크롤링 간격을 3~5초 사이로 설정\n",
        "            elif response.status_code == 403:\n",
        "                print(\"403 에러 발생, 요청을 건너뜁니다.\")\n",
        "                sleep(random.uniform(60, 90))  # 짧게 대기 후 다음 요청\n",
        "                continue\n",
        "            else:\n",
        "                print(f\"Error fetching page: {response.status_code}\")\n",
        "                sleep(random.uniform(60, 90))\n",
        "                continue\n",
        "        except Exception as e:\n",
        "            print(f\"예상치 못한 에러 발생: {e}\")\n",
        "            sleep(random.uniform(60, 90))\n",
        "            continue\n",
        "\n",
        "    # 뉴스 본문 크롤링\n",
        "    print(\"뉴스 본문 텍스트 크롤링을 시작합니다!\")\n",
        "    for idx, link in enumerate(naver_news_links):\n",
        "        try:\n",
        "            response = get(link, headers=headers)\n",
        "            if response.status_code == 200:\n",
        "                soup = bs(response.text, 'html.parser')\n",
        "                title = soup.select_one('div.media_end_head_title').text.strip().replace('\\n', '')\n",
        "                content = soup.select_one('article#dic_area').text.strip().replace('\\n', '')\n",
        "                article_date = soup.select_one('span.media_end_head_info_datestamp_time').get_text(strip=True)\n",
        "                press = soup.select_one('div.media_end_head_top a').text.strip()\n",
        "\n",
        "                # 언론사명 중복 제거 (예: \"경향일보경향일보\" -> \"경향일보\")\n",
        "                press = re.sub(r'\\b(\\w+)\\b\\s*\\1', r'\\1', press)\n",
        "\n",
        "                # 데이터를 클린한 후 엑셀 파일에 추가\n",
        "                ws.append([\n",
        "                    clean_string(article_date),\n",
        "                    clean_string(keyword),\n",
        "                    clean_string(title),\n",
        "                    clean_string(content),\n",
        "                    clean_string(link),\n",
        "                    clean_string(press)\n",
        "                ])\n",
        "\n",
        "                print(f\"뉴스 제목: {title} {idx + 1}/{len(naver_news_links)}\")\n",
        "                sleep(random.uniform(2, 4))  # 본문 크롤링 간격 설정\n",
        "            else:\n",
        "                print(f\"Error fetching article: {response.status_code}\")\n",
        "                sleep(random.uniform(2, 4))\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching article at {link}: {e}\")\n",
        "            sleep(random.uniform(2, 4))\n",
        "\n",
        "# 날짜 범위를 나누어 크롤링 실행\n",
        "date_ranges = get_date_ranges(start_date, end_date)\n",
        "for start, end in date_ranges:\n",
        "    fetch_news_by_date_range(keyword, start, end)\n",
        "\n",
        "# 엑셀 파일 저장\n",
        "today_str = date.today().strftime(\"%Y%m%d\")\n",
        "wb.save(f'naver_news_{keyword}_{start_date}_{end_date}_{today_str}.xlsx')\n",
        "print(\"Crawling and data extraction complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
