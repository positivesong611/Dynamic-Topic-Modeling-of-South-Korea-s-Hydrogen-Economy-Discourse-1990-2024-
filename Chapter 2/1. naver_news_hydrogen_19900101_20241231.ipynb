{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSE5h2YEkCwY",
        "outputId": "c34e2aa0-0b73-4323-bd26-a3cd0247bb8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "í˜ì´ì§€ 1 í¬ë¡¤ë§ ì¤‘ì…ë‹ˆë‹¤.\n",
            "ë‰´ìŠ¤ ë³¸ë¬¸ í…ìŠ¤íŠ¸ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤!\n",
            "í˜ì´ì§€ 1 í¬ë¡¤ë§ ì¤‘ì…ë‹ˆë‹¤.\n",
            "ë‰´ìŠ¤ ë³¸ë¬¸ í…ìŠ¤íŠ¸ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤!\n",
            "í˜ì´ì§€ 1 í¬ë¡¤ë§ ì¤‘ì…ë‹ˆë‹¤.\n",
            "ë‰´ìŠ¤ ë³¸ë¬¸ í…ìŠ¤íŠ¸ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤!\n",
            "í˜ì´ì§€ 1 í¬ë¡¤ë§ ì¤‘ì…ë‹ˆë‹¤.\n",
            "ë‰´ìŠ¤ ë³¸ë¬¸ í…ìŠ¤íŠ¸ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤!\n",
            "í˜ì´ì§€ 1 í¬ë¡¤ë§ ì¤‘ì…ë‹ˆë‹¤.\n",
            "ë‰´ìŠ¤ ë³¸ë¬¸ í…ìŠ¤íŠ¸ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤!\n",
            "í˜ì´ì§€ 1 í¬ë¡¤ë§ ì¤‘ì…ë‹ˆë‹¤.\n",
            "ë‰´ìŠ¤ ë³¸ë¬¸ í…ìŠ¤íŠ¸ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤!\n",
            "í˜ì´ì§€ 1 í¬ë¡¤ë§ ì¤‘ì…ë‹ˆë‹¤.\n",
            "ë‰´ìŠ¤ ë³¸ë¬¸ í…ìŠ¤íŠ¸ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤!\n",
            "í˜ì´ì§€ 1 í¬ë¡¤ë§ ì¤‘ì…ë‹ˆë‹¤.\n",
            "ë‰´ìŠ¤ ë³¸ë¬¸ í…ìŠ¤íŠ¸ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤!\n",
            "í˜ì´ì§€ 1 í¬ë¡¤ë§ ì¤‘ì…ë‹ˆë‹¤.\n",
            "ë‰´ìŠ¤ ë³¸ë¬¸ í…ìŠ¤íŠ¸ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤!\n",
            "í˜ì´ì§€ 1 í¬ë¡¤ë§ ì¤‘ì…ë‹ˆë‹¤.\n",
            "ë‰´ìŠ¤ ë³¸ë¬¸ í…ìŠ¤íŠ¸ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤!\n",
            "í˜ì´ì§€ 1 í¬ë¡¤ë§ ì¤‘ì…ë‹ˆë‹¤.\n",
            "ë‰´ìŠ¤ ë³¸ë¬¸ í…ìŠ¤íŠ¸ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤!\n",
            "í˜ì´ì§€ 1 í¬ë¡¤ë§ ì¤‘ì…ë‹ˆë‹¤.\n",
            "ë‰´ìŠ¤ ë³¸ë¬¸ í…ìŠ¤íŠ¸ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤!\n",
            "í˜ì´ì§€ 1 í¬ë¡¤ë§ ì¤‘ì…ë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "# Description: Crawls Naver News for articles containing a given keyword (e.g., \"ìˆ˜ì†Œ\") from 1990 to 2024 and exports the titles, content, and metadata to Excel.\n",
        "\n",
        "import re\n",
        "from requests import get\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from datetime import datetime, timedelta, date\n",
        "from time import sleep\n",
        "import random\n",
        "from openpyxl import Workbook\n",
        "\n",
        "# ===============================\n",
        "# ğŸ”¹ User Input\n",
        "# ===============================\n",
        "keyword = 'ìˆ˜ì†Œ'  # Search keyword\n",
        "start_date = '1990.01.01'  # Start date\n",
        "end_date = '2024.12.31'    # End date\n",
        "\n",
        "# ===============================\n",
        "# ğŸ”¹ Create Excel Workbook\n",
        "# ===============================\n",
        "wb = Workbook()\n",
        "ws = wb.active\n",
        "ws.title = keyword\n",
        "\n",
        "# Set column widths\n",
        "ws.column_dimensions['A'].width = 20\n",
        "ws.column_dimensions['B'].width = 20\n",
        "ws.column_dimensions['C'].width = 60\n",
        "ws.column_dimensions['D'].width = 120\n",
        "ws.column_dimensions['E'].width = 50\n",
        "ws.column_dimensions['F'].width = 20  # News outlet\n",
        "\n",
        "# Write header row\n",
        "ws.append(['ë‚ ì§œ', 'í‚¤ì›Œë“œ', 'ë„¤ì´ë²„ ë‰´ìŠ¤ì œëª©', 'ë„¤ì´ë²„ë‰´ìŠ¤ ë‚´ìš©', 'ë„¤ì´ë²„ë‰´ìŠ¤ ë§í¬', 'ì–¸ë¡ ì‚¬ëª…'])\n",
        "\n",
        "# ===============================\n",
        "# ğŸ”¹ HTTP headers for web request\n",
        "# ===============================\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',\n",
        "    'Connection': 'keep-alive',\n",
        "    'Upgrade-Insecure-Requests': '1',\n",
        "    'Referer': 'https://www.naver.com/',\n",
        "}\n",
        "\n",
        "# ===============================\n",
        "# ğŸ”¹ Split the date range into 3-month chunks\n",
        "# ===============================\n",
        "def get_date_ranges(start_date, end_date, delta=timedelta(days=90)):\n",
        "    start_dt = datetime.strptime(start_date, '%Y.%m.%d')\n",
        "    end_dt = datetime.strptime(end_date, '%Y.%m.%d')\n",
        "\n",
        "    ranges = []\n",
        "    while start_dt < end_dt:\n",
        "        temp_end_dt = min(start_dt + delta, end_dt)\n",
        "        ranges.append((start_dt.strftime('%Y.%m.%d'), temp_end_dt.strftime('%Y.%m.%d')))\n",
        "        start_dt = temp_end_dt + timedelta(days=1)\n",
        "    return ranges\n",
        "\n",
        "# ===============================\n",
        "# ğŸ”¹ Clean illegal characters from text for Excel\n",
        "# ===============================\n",
        "def clean_string(value):\n",
        "    ILLEGAL_CHARACTERS_RE = re.compile(r'[\\x00-\\x1F\\x7F-\\x9F\\uD800-\\uDFFF]')\n",
        "    if isinstance(value, str):\n",
        "        return ILLEGAL_CHARACTERS_RE.sub('', value)\n",
        "    return value\n",
        "\n",
        "# ===============================\n",
        "# ğŸ”¹ Crawl news links and article content\n",
        "# ===============================\n",
        "def fetch_news_by_date_range(keyword, start_date, end_date):\n",
        "    naver_news_links = []\n",
        "\n",
        "    # Iterate over search result pages (up to 2000 results)\n",
        "    for page in range(1, 3001, 10):\n",
        "        print(f\"í¬ë¡¤ë§ ì¤‘: í˜ì´ì§€ {page // 10 + 1}\")\n",
        "        url = f\"https://search.naver.com/search.naver?where=news&query={keyword}&sm=tab_opt&sort=0&photo=0&field=0&pd=3&ds={start_date}&de={end_date}&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3Afrom{start_date.replace('.', '')}to{end_date.replace('.', '')}&start={page}\"\n",
        "\n",
        "        try:\n",
        "            response = get(url, headers=headers)\n",
        "            if response.status_code == 200:\n",
        "                soup = bs(response.text, 'html.parser')\n",
        "                news_items = soup.select('a.info')\n",
        "                if not news_items:\n",
        "                    break\n",
        "                for item in news_items:\n",
        "                    link = item['href']\n",
        "                    if 'news.naver.com' in link:\n",
        "                        naver_news_links.append(link)\n",
        "                sleep(random.uniform(3, 5))\n",
        "            elif response.status_code == 403:\n",
        "                print(\"403 ì—ëŸ¬ ë°œìƒ, ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
        "                sleep(random.uniform(60, 90))\n",
        "                continue\n",
        "            else:\n",
        "                print(f\"Error fetching page: {response.status_code}\")\n",
        "                sleep(random.uniform(60, 90))\n",
        "                continue\n",
        "        except Exception as e:\n",
        "            print(f\"ì˜ˆì™¸ ë°œìƒ: {e}\")\n",
        "            sleep(random.uniform(60, 90))\n",
        "            continue\n",
        "\n",
        "    # Crawl article content\n",
        "    print(\"ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì‹œì‘\")\n",
        "    for idx, link in enumerate(naver_news_links):\n",
        "        try:\n",
        "            response = get(link, headers=headers)\n",
        "            if response.status_code == 200:\n",
        "                soup = bs(response.text, 'html.parser')\n",
        "                title = soup.select_one('div.media_end_head_title').text.strip().replace('\\n', '')\n",
        "                content = soup.select_one('article#dic_area').text.strip().replace('\\n', '')\n",
        "                article_date = soup.select_one('span.media_end_head_info_datestamp_time').get_text(strip=True)\n",
        "                press = soup.select_one('div.media_end_head_top a').text.strip()\n",
        "\n",
        "                # Remove duplicate press names\n",
        "                press = re.sub(r'\\b(\\w+)\\b\\s*\\1', r'\\1', press)\n",
        "\n",
        "                # Append to Excel\n",
        "                ws.append([\n",
        "                    clean_string(article_date),\n",
        "                    clean_string(keyword),\n",
        "                    clean_string(title),\n",
        "                    clean_string(content),\n",
        "                    clean_string(link),\n",
        "                    clean_string(press)\n",
        "                ])\n",
        "\n",
        "                print(f\"ê¸°ì‚¬ ì €ì¥ ì™„ë£Œ: {title} ({idx + 1}/{len(naver_news_links)})\")\n",
        "                sleep(random.uniform(2, 4))\n",
        "            else:\n",
        "                print(f\"ë³¸ë¬¸ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}\")\n",
        "                sleep(random.uniform(2, 4))\n",
        "        except Exception as e:\n",
        "            print(f\"ë³¸ë¬¸ í¬ë¡¤ë§ ì‹¤íŒ¨ ({link}): {e}\")\n",
        "            sleep(random.uniform(2, 4))\n",
        "\n",
        "# ===============================\n",
        "# ğŸ”¹ Run crawler over all date intervals\n",
        "# ===============================\n",
        "date_ranges = get_date_ranges(start_date, end_date)\n",
        "for start, end in date_ranges:\n",
        "    fetch_news_by_date_range(keyword, start, end)\n",
        "\n",
        "# ===============================\n",
        "# ğŸ”¹ Save Excel file\n",
        "# ===============================\n",
        "today_str = date.today().strftime(\"%Y%m%d\")\n",
        "filename = f'naver_news_hydrogen_{start_date.replace(\".\", \"\")}_{end_date.replace(\".\", \"\")}_{today_str}.xlsx'\n",
        "wb.save(filename)\n",
        "print(f\"âœ… Crawling complete. File saved as: {filename}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
